{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99882fb6-33c7-4074-9358-ff0005850e3c",
   "metadata": {},
   "source": [
    "# Interactive visualization of all possible first tokens of FlexTok finetuned on CelebA-HQ\n",
    "\n",
    "The first token in the FlexTok sequence captures the most essential high-level information about an image. Use this notebook to interactively explore different first token values and see how they affect the image reconstruction. Each slider corresponds to one FSQ level, and for each token index we show 9 random samples from the FlexTok d18-d28 decoder. When we consider only the first token, there are 64000 possible indices created by the FSQ levels [8,8,8,5,5,5]. In essence, that means FlexTok partitions the distribution of all possible images into 64000 clusters, each represented by a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cdcf89-3f06-440c-9f31-1a5260706ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch path to root of project\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde45588-a379-494f-9d3e-2b28366130ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "BF16 enabled: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iyu/miniconda3/envs/flextok/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from diffusers.models import AutoencoderKL\n",
    "\n",
    "from flextok.flextok_wrapper import FlexTokFromHub, FlexTok\n",
    "from flextok.utils.demo import imgs_from_urls, denormalize, batch_to_pil\n",
    "from flextok.utils.misc import detect_bf16_support, get_bf16_context, get_generator\n",
    "from flextok.utils.dataloader import CelebAHQDataset, create_celebahq_dataloader\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Global no_grad\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Automatically set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "# Detect if bf16 is enabled or not\n",
    "enable_bf16 = detect_bf16_support()\n",
    "print('BF16 enabled:', enable_bf16)\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54835b6a-49cf-4a84-a66a-8f89538f47a1",
   "metadata": {},
   "source": [
    "## 1 Sampling tokens\n",
    "\n",
    "The FlexTok encoder maps an image into a one-dimensional sequence of 256 register tokens. These tokens are discretized using FSQ, resulting in a vocabulary of 64,000 tokens.\n",
    "\n",
    "FlexTok was trained so that these quantized register tokens represent images in a hierarchical and ordered manner. Specifically, truncated subsequences of lengths _1, 2, 4, 8, 16, 32, 64, 128, and 256_ tokens all represent valid images, and as more tokens are used, the reconstructions increasingly resemble the encoded image. Although FlexTok was trained solely using a rectified flow and REPA objective, the truncated token sequences emerge as highly semantic compressions of the original image, capturing its most salient aspects with the fewest tokens.\n",
    "\n",
    "Here, I'm loading a FlexTok d18 d18 model trained on ImageNet1K and finetuned on CelebA-HQ from a local checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223827dc-7e03-48c2-af73-5c64c6269894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a FlexTok d18-d18 model trained on ImageNet1K and finetuned on CelebA-HQ from a local checkpoint.\n",
    "flextok = FlexTokFromHub.from_pretrained('EPFL-VILAB/flextok_d18_d18_in1k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734a225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/home/iyu/ml-flextok/checkpoints/celebahq_ft/checkpoint_latest.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "flextok.load_state_dict(checkpoint['model_state_dict'])\n",
    "flextok = flextok.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209b237-f0e0-4101-8d63-9b029d9476b1",
   "metadata": {},
   "source": [
    "### 1.1 Sampling possible first tokens\n",
    "This is an interactive GUI for exploring the possible values for the first token of FlexTok fine-tuned on CelebA-HQ. Use the sliders to explore each FSQ level (the FSQ levels are [8, 8, 8, 5, 5, 5]).\n",
    "\n",
    "First, we write functions to sample random quantization levels and convert those levels into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44fe1d87-6ecb-4464-8b34-fe445cf67def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def get_possible_combos(flextok_model: FlexTok):\n",
    "    \"\"\"\n",
    "    Get all possible first zhats (quantized latents) from the FlexTok model.\n",
    "    Args:\n",
    "        flextok_model: The FlexTok model.\n",
    "        num_samples: Number of samples to generate.\n",
    "    Returns:\n",
    "        batch of first zhats (num_samples, d).\n",
    "    \"\"\"\n",
    "    # Get the FSQ from flextok model\n",
    "    fsq = flextok_model.regularizer\n",
    "    fsq_levels = fsq._levels  # e.g., [8, 8, 8, 5, 5, 5]\n",
    "    print(\"FSQ levels:\", fsq_levels)\n",
    "    print(\"codebook size:\", fsq.codebook_size)\n",
    "\n",
    "    quantizations = [torch.linspace(-1, 1, steps=L) for L in fsq_levels]\n",
    "    all_combinations = list(product(*quantizations))\n",
    "    print(\"Total combinations (must equal codebook size):\", len(all_combinations))\n",
    "    \n",
    "    return torch.stack([torch.tensor(comb) for comb in all_combinations], dim=0)\n",
    "\n",
    "def zhat_to_tokens(flextok_model: FlexTok, zhats: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given list of zhats, generate tokens from zhats.\n",
    "    Args:\n",
    "        flextok_model: The FlexTok model.\n",
    "        zhats: zhats (N, d).\n",
    "    Returns:\n",
    "        tokens (N, 1).\n",
    "    \"\"\"\n",
    "    fsq = flextok_model.regularizer\n",
    "    tokens = fsq.codes_to_indices(zhats)  # (N, 1)\n",
    "    print(\"tokens shape:\", tokens.shape)\n",
    "    return tokens.long()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138fc45-b655-4a51-b343-d1a39ce5025e",
   "metadata": {},
   "source": [
    "Then, we detokenize these tokens back into images using the FlexTok rectified flow decoder. There are three important hyperparameters for the rectified flow decoder:\n",
    "\n",
    "- `timesteps`: Number of denoising steps. 25 steps provides a good balance between reconstruction quality and inference speed.\n",
    "- `guidance_scale`: Classifier-free guidance scale. See the paper appendix for guidance scale sweeps for all models. We recommend guidance scale 7.5, except for the FlexTok d12-d12 model where we recommend guidance scale 15.\n",
    "- `perform_norm_guidance`: Whether or not to perform Adaptive Projected Guidance (APG), see https://arxiv.org/abs/2410.02416. We recommend setting this to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080fb710-6692-4f11-8879-5a47a6bffe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSQ levels: tensor([8, 8, 8, 5, 5, 5], device='cuda:0', dtype=torch.int32)\n",
      "codebook size: 64000\n",
      "Total combinations (must equal codebook size): 64000\n",
      "tokens shape: torch.Size([64000])\n",
      "zhats tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -0.5000, -1.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -0.5000, -0.5000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -0.5000,  0.0000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -0.5000,  0.5000],\n",
      "        [-1.0000, -1.0000, -1.0000, -1.0000, -0.5000,  1.0000]],\n",
      "       device='cuda:0')\n",
      "tokens list (tensor([[0]], device='cuda:0'), tensor([[12800]], device='cuda:0'), tensor([[25600]], device='cuda:0'), tensor([[38400]], device='cuda:0'), tensor([[51200]], device='cuda:0'), tensor([[2560]], device='cuda:0'), tensor([[15360]], device='cuda:0'), tensor([[28160]], device='cuda:0'), tensor([[40960]], device='cuda:0'), tensor([[53760]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "zhats = get_possible_combos(flextok).to(device)  # (64000, 6)\n",
    "tokens_list = zhat_to_tokens(flextok, zhats).unsqueeze(-1)  # (64000, 1)\n",
    "tokens_list = tokens_list.split(1)  # list of (1, 1) tensors\n",
    "print(\"zhats\", zhats[:10])\n",
    "print(\"tokens list\", tokens_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4045f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSQ levels: tensor([8, 8, 8, 5, 5, 5], device='cuda:0', dtype=torch.int32)\n",
      " 24%|████████████████████████████████▎                                                                                                   | 489/2000 [13:22:06<37:30:22, 89.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                    | 0/2000 [00:00<?, ?it/s]/home/iyu/ml-flextok/flextok/model/utils/posembs.py:124: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  posembs = posembs[slices]\n",
      "/home/iyu/ml-flextok/flextok/model/utils/posembs.py:124: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:345.)\n",
      "  posembs = posembs[slices]\n",
      "  1%|▉                                                                                                                                     | 14/2000 [54:15<126:26:47, 229.21s/it]"
     ]
    }
   ],
   "source": [
    "# now detokenize in batches to avoid OOM\n",
    "num_samples_per_quantization_combo = 9\n",
    "batch_size = 32\n",
    "fsq_levels = flextok.regularizer._levels\n",
    "print(\"FSQ levels:\", fsq_levels)\n",
    "\n",
    "import tqdm\n",
    "img_output_dir = \"/home/iyu/flextok_first_token_samples/\"\n",
    "os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(tokens_list), batch_size)):\n",
    "    batch_tokens_list = tokens_list[i:i+batch_size]  # list of (1, 1) tensors\n",
    "    batch_zhats = zhats[i:i+batch_size]  # (B, 6)\n",
    "    for sample in range(num_samples_per_quantization_combo):\n",
    "        with get_bf16_context(enable_bf16):\n",
    "            with torch.no_grad():\n",
    "                reconst = flextok.detokenize(\n",
    "                    batch_tokens_list,\n",
    "                    timesteps=25, # Number of denoising steps\n",
    "                    guidance_scale=7.5, # Classifier-free guidance scale\n",
    "                    perform_norm_guidance=True, # APG, see https://arxiv.org/abs/2410.02416\n",
    "                    # Optionally control initial noise. Note that while the initial noise is deterministic, the rest of the model isn't.\n",
    "                    generator=None,\n",
    "                    verbose=False, # Enable to show denoising progress bar with tqdm\n",
    "                )\n",
    "        # save image samples to disk to avoid OOM\n",
    "        for j in range(reconst.shape[0]):\n",
    "            img = reconst[j]\n",
    "            img = denormalize(img).clamp(0, 1)\n",
    "            img_pil = TF.to_pil_image(img.cpu())\n",
    "            zhat_tuple = tuple(batch_zhats[j].cpu().numpy())\n",
    "            save_path = os.path.join(img_output_dir, f\"quant_{'_'.join([str(v) for idx, v in enumerate(zhat_tuple)])}_sample_{sample+1}.png\")\n",
    "            img_pil.save(save_path)\n",
    "        del reconst  # free memory\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "udzi05l20b",
   "metadata": {},
   "source": [
    "## 2 Interactive GUI for Exploring First Tokens\n",
    "\n",
    "Use the sliders below to explore different quantization combinations for the first token. Each slider corresponds to one FSQ dimension, and the GUI displays 9 random samples for the selected quantization combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9mxgne86ozg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c96547edfdd4db7aba192b611cb0fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=4, continuous_update=False, description='Dim 0:', layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# FSQ levels configuration\n",
    "fsq_levels = [8, 8, 8, 5, 5, 5]\n",
    "img_output_dir = \"/home/iyu/flextok_first_token_samples/\"\n",
    "\n",
    "# Create the quantization value mappings for each level\n",
    "def get_quant_values(level):\n",
    "    \"\"\"Get quantization values for a given FSQ level\"\"\"\n",
    "    return torch.linspace(-1, 1, steps=level).tolist()\n",
    "\n",
    "# Pre-compute all quantization values\n",
    "quant_values_per_level = [get_quant_values(level) for level in fsq_levels]\n",
    "\n",
    "def load_and_display_images(slider_values):\n",
    "    \"\"\"Load and display images for the selected quantization combination\"\"\"\n",
    "    # Convert slider indices to actual quantization values\n",
    "    quant_combo = [quant_values_per_level[i][slider_values[i]] for i in range(len(fsq_levels))]\n",
    "    \n",
    "    # Format the quantization combo for filename\n",
    "    quant_str = \"_\".join([str(float(v)) for v in quant_combo])\n",
    "    \n",
    "    # Load the 9 sample images\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    images_found = 0\n",
    "    for sample_num in range(1, 10):\n",
    "        img_path = os.path.join(img_output_dir, f\"quant_{quant_str}_sample_{sample_num}.png\")\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            axes[sample_num - 1].imshow(img)\n",
    "            axes[sample_num - 1].axis('off')\n",
    "            images_found += 1\n",
    "        else:\n",
    "            axes[sample_num - 1].text(0.5, 0.5, 'Not found', \n",
    "                                     ha='center', va='center', fontsize=12)\n",
    "            axes[sample_num - 1].axis('off')\n",
    "    \n",
    "    # Title showing the quantization values\n",
    "    fig.suptitle(f'Quantization: [{\", \".join([f\"{v:.2f}\" for v in quant_combo])}]', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if images_found == 0:\n",
    "        print(f\"Warning: No images found for quantization {quant_combo}\")\n",
    "        print(f\"Looking for pattern: quant_{quant_str}_sample_*.png\")\n",
    "\n",
    "# Create sliders for each FSQ level\n",
    "sliders = []\n",
    "slider_labels = []\n",
    "\n",
    "for i, level in enumerate(fsq_levels):\n",
    "    # Get the quantization values for this level\n",
    "    quant_vals = quant_values_per_level[i]\n",
    "    \n",
    "    # Create slider\n",
    "    slider = widgets.IntSlider(\n",
    "        value=level // 2,  # Start at middle value\n",
    "        min=0,\n",
    "        max=level - 1,\n",
    "        step=1,\n",
    "        description=f'Dim {i}:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "    \n",
    "    # Create label showing the actual quantization value\n",
    "    label = widgets.Label(value=f'{quant_vals[slider.value]:.2f}')\n",
    "    \n",
    "    # Update label when slider changes\n",
    "    def make_update_label(slider, label, quant_vals):\n",
    "        def update_label(change):\n",
    "            label.value = f'{quant_vals[change.new]:.2f}'\n",
    "        return update_label\n",
    "    \n",
    "    slider.observe(make_update_label(slider, label, quant_vals), names='value')\n",
    "    \n",
    "    sliders.append(slider)\n",
    "    slider_labels.append(label)\n",
    "\n",
    "# Create output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_slider_change(change):\n",
    "    \"\"\"Update display when any slider changes\"\"\"\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        slider_values = [s.value for s in sliders]\n",
    "        load_and_display_images(slider_values)\n",
    "\n",
    "# Attach the update function to all sliders\n",
    "for slider in sliders:\n",
    "    slider.observe(on_slider_change, names='value')\n",
    "\n",
    "# Create the UI layout\n",
    "slider_boxes = [widgets.HBox([slider, label]) for slider, label in zip(sliders, slider_labels)]\n",
    "ui = widgets.VBox(slider_boxes + [output])\n",
    "\n",
    "# Display the UI\n",
    "display(ui)\n",
    "\n",
    "# Initial display\n",
    "with output:\n",
    "    slider_values = [s.value for s in sliders]\n",
    "    load_and_display_images(slider_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlexTok (flextok)",
   "language": "python",
   "name": "flextok"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
