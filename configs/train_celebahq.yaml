# FlexTok Fine-tuning Configuration for CelebA-HQ
#
# This configuration file defines hyperparameters for fine-tuning FlexTok
# on the CelebA-HQ dataset.

# ============================================================================
# Model Configuration
# ============================================================================

# Pre-trained model to load from Hugging Face
model_name: "EPFL-VILAB/flextok_d18_d18_in1k"

# Which components to train
train_encoder: true
train_decoder: true
train_vae: false

train_from_scratch: false  # If true, ignores pretrained weights

# Binary quantization levels for FlexTok (null to use all levels)
fsq_levels: null

gradient_checkpointing: true  # Enable gradient checkpointing to save memory

# ============================================================================
# Data Configuration
# ============================================================================

# Path to dataset
# Expected structure: data_path/{00000.jpg, 00001.jpg, ...}
# or data_path/images/{00000.jpg, 00001.jpg, ...}
dataset_name: "celebahq"
data_path: "./data/${dataset_name}"

# Image size (256, 512, or 1024 depending on your dataset)
img_size: 256

# Data augmentation
use_augmentation: true  # Enables RandomHorizontalFlip

# ============================================================================
# Training Configuration
# ============================================================================

# Number of training epochs
num_epochs: 50

# Batch sizes
batch_size: 16
val_batch_size: 64

# Maximum validation batches (null for all batches, set to limit for memory)
# Example: 50 means validate on only first 50 batches
max_val_batches: null

# Number of data loading workers
num_workers: 4

# Gradient accumulation steps (effective batch size = batch_size * gradient_accumulation_steps)
# With batch_size=16 and gradient_accumulation_steps=4, effective batch size = 64
gradient_accumulation_steps: 1

# Mixed precision training (speeds up training, reduces memory)
use_amp: true

# Gradient clipping (1.0 is recommended for stable training)
max_grad_norm: 1.0

# ============================================================================
# Optimizer Configuration
# ============================================================================

# Optimizer type: adamw or adam
optimizer: "adamw"

# Learning rates (can set different LRs for different components)
learning_rate: 1.0e-6
encoder_lr: 1.0e-6
decoder_lr: 1.0e-6
vae_lr: 1.0e-6  # Lower LR for VAE if training it

# AdamW parameters
betas: [0.9, 0.999]
weight_decay: 0.01
eps: 1.0e-8

# ============================================================================
# Learning Rate Scheduler
# ============================================================================

# Scheduler type: cosine, linear, or none
scheduler: "cosine"

# Warmup steps
warmup_steps: 200

# Minimum learning rate for cosine scheduler
min_lr: 1.0e-6

# ============================================================================
# EMA (Exponential Moving Average)
# ============================================================================

# Use EMA for more stable inference
use_ema: false
ema_decay: 0.999

# ============================================================================
# Inference Configuration
# ============================================================================

# Number of denoising steps for reconstruction during validation
inference_steps: 20

# Guidance scale for decoder (classifier-free guidance)
guidance_scale: 7.5

# Whether to perform Adaptive Projected Guidance (APG)
perform_norm_guidance: true

# Token counts to visualize in flexible-length reconstructions
# Shows how the model progressively builds up the image with more tokens
vis_token_counts: [1, 2, 3, 4, 16, 256]

# ============================================================================
# Checkpointing
# ============================================================================

# Directory to save checkpoints
checkpoint_dir: "./checkpoints/celebahq_ft"
resume: null  # Path to checkpoint to resume from, null to start fresh

# Save checkpoint every N epochs
save_every: 1

# ============================================================================
# Logging & Visualization
# ============================================================================

# Wandb configuration
use_wandb: true
wandb_project: "flextok-finetuning"
wandb_run_name: "celebahq-ft-256"  # Set to null for auto-generated name
wandb_tags: ["celebahq", "finetuning", "flextok"]

# Log metrics every N steps
log_every: 10

# Visualize reconstructions every N steps
visualize_every: 1000

# ============================================================================
# REPA Loss Configuration
# ============================================================================

# Enable REPA auxiliary loss for better semantic representations
use_repa: false

# Weight for REPA loss (paper recommends 1.0 for equal weighting)
repa_weight: 1.0

# Pretrained encoder for REPA
# Options: 'dinov2_vitl14', 'dinov2_vitg14', 'dinov2_vitb14'
repa_encoder_type: 'dinov2_vitl14'

# Encoder output dimension
# DINOv2-L: 1024, DINOv2-G: 1536, DINOv2-B: 768
repa_encoder_dim: 1024

# Target spatial size for feature matching (paper uses 37x37)
repa_target_size: [37, 37]

# Separate learning rate for REPA projector (optional)
repa_lr: 1.0e-4
